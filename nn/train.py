from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras import callbacks

from params import *
from data import prepare_data
from model import build_model

# data
x_train, y_train, x_test, y_test = prepare_data()

# create callbacks
csv_logger = callbacks.CSVLogger('training.log')
reduce_lr =  callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,
                                         patience=10, min_lr=0.001)
all_callbacks = [csv_logger, reduce_lr]

# build the model
model = build_model(x_train.shape[1:])

# optimizer
sgd = optimizers.SGD(lr=0.2, momentum=0.9, decay=1e-4)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

# train the model with image augmentation, this part comes from the offical tutorial
print('Using real-time data augmentation.')
datagen = ImageDataGenerator( 
               featurewise_center=False,             # set input mean to 0 over the dataset
               samplewise_center=False,              # set each sample mean to 0
               featurewise_std_normalization=False,  # divide inputs by std of the dataset
               samplewise_std_normalization=False,   # divide each input by its std
               zca_whitening=False,                  # apply ZCA whitening
               rotation_range=0,                     # randomly rotate images in the range (degrees, 0 to 180)
               width_shift_range=0.1,                # randomly shift images horizontally (fraction of total width)
               height_shift_range=0.1,               # randomly shift images vertically (fraction of total height)
               horizontal_flip=True,                 # randomly flip images
               vertical_flip=False)                  # randomly flip images

# Compute quantities required for feature-wise normalization
datagen.fit(x_train)

# Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(x_train, y_train,
                                 batch_size=batch_size),
                                 steps_per_epoch=x_train.shape[0] // batch_size,
                                 epochs=epochs,
                                 validation_data=(x_test, y_test),
                                 callbacks=all_callbacks)
